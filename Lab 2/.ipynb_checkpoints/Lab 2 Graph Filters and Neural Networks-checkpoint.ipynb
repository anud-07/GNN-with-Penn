{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900262db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abc2bb",
   "metadata": {},
   "source": [
    "# Stochastic Block Model (SBM) Graph Generation via the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3440b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbm(n, c, p_intra, p_inter):\n",
    "    \n",
    "    # assign a community to each node\n",
    "    community = np.repeat(list(range(c)), np.ceil(n / c))\n",
    "\n",
    "    # make sure community vector has size n\n",
    "    community = community[0:n]\n",
    "\n",
    "    # make it a column vector\n",
    "    community = np.expand_dims(community, 1)\n",
    "\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in the same community\n",
    "    intra = community == community.T\n",
    "\n",
    "    # generate a boolean matrix indicating whether two nodes \n",
    "    # are in different communities\n",
    "    inter = np.logical_not(intra)\n",
    "\n",
    "    # generate a matrix with random entries between 0 and 1\n",
    "    random = np.random.random((n, n))\n",
    "\n",
    "    # generate a triangular matrix with zeros below the main diagonal\n",
    "    # because the SBM graph is symmetric, we only have to assign weights \n",
    "    # to the upper triangular part of the adjacency matrix,\n",
    "    # and later copy them to the lower triangular part\n",
    "    tri = np.tri(n, k=-1)\n",
    "\n",
    "\n",
    "    # initialize adjacency matrix\n",
    "    graph = np.zeros((n, n))\n",
    "\n",
    "    # assign intra-community edges\n",
    "    graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1\n",
    "\n",
    "    # assign inter-community edges\n",
    "    graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1\n",
    "\n",
    "    # make the adjacency matrix symmetric\n",
    "    graph += graph.T \n",
    "\n",
    "    return graph\n",
    "\n",
    "def normalize_gso(gso):\n",
    "    \n",
    "    # obtain eigenvalues\n",
    "    eigenvalues, _ = np.linalg.eig(gso) \n",
    "\n",
    "    # normalize by eigenvalue with largest absolute value\n",
    "    return gso / np.max(np.abs(eigenvalues))\n",
    "\n",
    "\n",
    "S = sbm(n=50, c=5, p_intra=0.6, p_inter=0.2)\n",
    "S = normalize_gso(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66088edd",
   "metadata": {},
   "source": [
    "# Data Generation and Training/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f705ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diffusion(gso, n_samples, n_sources):\n",
    "\n",
    "    # get the number of nodes\n",
    "    n = gso.shape[0]\n",
    "\n",
    "    # initialize the tensor used to store the samples\n",
    "    # shape is n_samples x n x time x 1 features\n",
    "    z = np.zeros((n_samples, n, 5, 1))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # pick n_sources at random from n nodes\n",
    "        sources = np.random.choice(n, n_sources, replace=False)\n",
    "\n",
    "        # define z_0 for each sample\n",
    "        z[i, sources, 0, 0] = np.random.uniform(0, 10, n_sources)\n",
    "\n",
    "    # noise mean and variance\n",
    "    mu = np.zeros(n)\n",
    "    sigma = np.eye(n) * 1e-3\n",
    "\n",
    "    for t in range(4):\n",
    "\n",
    "        # generate noise\n",
    "        noise = np.random.multivariate_normal(mu, sigma, n_samples)\n",
    "\n",
    "        # generate z_t\n",
    "        print(z[:, :, t].shape)\n",
    "        z[:, :, t + 1] = gso @ z[:, :, t] + np.expand_dims(noise, -1)\n",
    "        \n",
    "    # transpose dimensions so shape is n_samples x time x n x 1 feature\n",
    "    z = z.transpose((0, 2, 1, 3))\n",
    "    \n",
    "    # squeeze feature dimension, as there is only 1 feature\n",
    "    return z.squeeze()\n",
    "\n",
    "# n_samples x time x n x 1\n",
    "z = generate_diffusion(S, 2100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_diffusion(z):\n",
    "    \n",
    "    # permute the samples in z\n",
    "    z = np.random.permutation(z)\n",
    "    \n",
    "    # define the output tensor\n",
    "    y = np.expand_dims(z[:, 0, :], 1)\n",
    "    print(y.shape)\n",
    "    \n",
    "    # initialize the input tensor\n",
    "    x = np.zeros(y.shape)\n",
    "    \n",
    "    # define the input tensor as x = z_4\n",
    "    for i, sample in enumerate(z):\n",
    "        x[i] = sample[4]\n",
    "   \n",
    "    print(x.shape)\n",
    "    # squeeze time dimension     \n",
    "    return x.squeeze(), y.squeeze()\n",
    "\n",
    "x, y = data_from_diffusion(z)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, splits = (2000, 100)):\n",
    "\n",
    "    # define the initial index of each set (training/test)\n",
    "    splits = np.cumsum([0] + list(splits))\n",
    "    splits = (splits * x.shape[0] / splits[-1]).astype(int)\n",
    "\n",
    "    # return training and test data as tuples\n",
    "    return ((x[splits[i]:splits[i + 1]], y[splits[i]:splits[i + 1]]) for i in range(len(splits) - 1))\n",
    "\n",
    "nTrain = 2000\n",
    "nTest = 100\n",
    "\n",
    "trainData, testData = split_data(x, y, (nTrain, nTest))\n",
    "xTrain = trainData[0]\n",
    "yTrain = trainData[1]\n",
    "xTest = testData[0]\n",
    "yTest = testData[1]\n",
    "\n",
    "xTrain = torch.tensor(xTrain)\n",
    "yTrain = torch.tensor(yTrain)\n",
    "xTest = torch.tensor(xTest)\n",
    "yTest = torch.tensor(yTest)\n",
    "\n",
    "print(xTrain.shape)\n",
    "print(yTrain.shape)\n",
    "print(xTest.shape)\n",
    "print(yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b7b74",
   "metadata": {},
   "source": [
    "# Implmenting the Graph Filter Architecture;\n",
    "\n",
    "We need functions that implements the shift-and-sum operation.\n",
    "\n",
    "We need functions such as the torch.nn.Module that defines the graph filter as a learning architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ade48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterFunction(h, S, x):\n",
    "    K = h.shape[0] # number of filter taps\n",
    "    B = x.shape[0] # batch size\n",
    "    N = x.shape[1] # number of nodes\n",
    "\n",
    "    x = x.reshape([B, 1, N])\n",
    "    S = S.reshape([1, N, N])\n",
    "    z = x\n",
    "\n",
    "    for k in range(1, K):\n",
    "\n",
    "        # diffusion step, S^k*x\n",
    "        x = torch.matmul(x, S)\n",
    "        xS = x.reshape([B, 1, N]) \n",
    "\n",
    "        # concatenate the S^k*x in the tensor z\n",
    "        z = torch.cat((z, xS), dim=1) \n",
    "\n",
    "    # multiply z and h in the concatenation dimension\n",
    "    y = torch.matmul(z.permute(0, 2, 1).reshape([B, N, K]), h)\n",
    "\n",
    "    return y\n",
    "\n",
    "class GraphFilter(nn.Module):\n",
    "    def __init__(self, gso, k):\n",
    "        \n",
    "        # Initialize parent\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save filter hyperparameters\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        \n",
    "        # Define and initialize learnable weights\n",
    "        self.weight = nn.Parameter(torch.randn(self.k))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return FilterFunction(self.weight, self.gso, x)\n",
    "    \n",
    "graphFilter = GraphFilter(S, 8)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee99407",
   "metadata": {},
   "source": [
    "#  loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaefd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6a5cc",
   "metadata": {},
   "source": [
    "# Training of the Graph filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a066bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationInterval = 5\n",
    "\n",
    "nEpochs = 30\n",
    "batchSize = 200\n",
    "learningRate = 0.05\n",
    "\n",
    "nValid = int(np.floor(0.01*nTrain))\n",
    "xValid = xTrain[0:nValid,:]\n",
    "yValid = yTrain[0:nValid,:]\n",
    "xTrain = xTrain[nValid:,:]\n",
    "yTrain = yTrain[nValid:,:]\n",
    "nTrain = xTrain.shape[0]\n",
    "\n",
    "# Declaring the optimizers for each architectures\n",
    "optimizer = optim.Adam(graphFilter.parameters(), lr=learningRate)\n",
    "\n",
    "if nTrain < batchSize:\n",
    "    nBatches = 1\n",
    "    batchSize = [nTrain]\n",
    "elif nTrain % batchSize != 0:\n",
    "    nBatches = np.ceil(nTrain/batchSize).astype(np.int64)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "    while sum(batchSize) != nTrain:\n",
    "        batchSize[-1] -= 1\n",
    "else:\n",
    "    nBatches = np.int(nTrain/batchSize)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "batchIndex = np.cumsum(batchSize).tolist()\n",
    "batchIndex = [0] + batchIndex\n",
    "\n",
    "epoch = 0 # epoch counter\n",
    "\n",
    "# Store the training...\n",
    "lossTrain = dict()\n",
    "lossValid = dict()\n",
    "# ...and test variables\n",
    "lossTestBest = dict()\n",
    "lossTestLast = dict()\n",
    "\n",
    "bestModel = dict()\n",
    "\n",
    "lossTrain = []\n",
    "lossValid = []\n",
    "    \n",
    "while epoch < nEpochs:\n",
    "    randomPermutation = np.random.permutation(nTrain)\n",
    "    idxEpoch = [int(i) for i in randomPermutation]\n",
    "    print(\"\")\n",
    "    print(\"Epoch %d\" % (epoch+1))\n",
    "\n",
    "    batch = 0 \n",
    "    \n",
    "    while batch < nBatches:\n",
    "        # Determine batch indices\n",
    "        thisBatchIndices = idxEpoch[batchIndex[batch]\n",
    "                                    : batchIndex[batch+1]]\n",
    "        \n",
    "        # Get the samples in this batch\n",
    "        xTrainBatch = xTrain[thisBatchIndices,:]\n",
    "        yTrainBatch = yTrain[thisBatchIndices,:]\n",
    "\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            print(\"\")\n",
    "            print(\"    (E: %2d, B: %3d)\" % (epoch+1, batch+1),end = ' ')\n",
    "            print(\"\")\n",
    "        \n",
    "       \n",
    "        # Reset gradients\n",
    "        graphFilter.zero_grad()\n",
    "\n",
    "        # Obtain the output of the architectures\n",
    "        yHatTrainBatch = graphFilter(xTrainBatch)\n",
    "\n",
    "        # Compute loss\n",
    "        lossValueTrain = loss(yHatTrainBatch, yTrainBatch)\n",
    "\n",
    "        # Compute gradients\n",
    "        lossValueTrain.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        lossTrain += [lossValueTrain.item()]\n",
    "        \n",
    "        # Print:\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            with torch.no_grad():\n",
    "                # Obtain the output of the GNN\n",
    "                yHatValid = graphFilter(xValid)\n",
    "    \n",
    "            # Compute loss\n",
    "            lossValueValid = loss(yHatValid, yValid)\n",
    "            \n",
    "            lossValid += [lossValueValid.item()]\n",
    "\n",
    "            print(\"\\t Graph Filter: %6.4f [T]\" % (\n",
    "                    lossValueTrain) + \" %6.4f [V]\" % (\n",
    "                    lossValueValid))\n",
    "            \n",
    "            # Saving the best model so far\n",
    "            if len(lossValid) > 1:\n",
    "                if lossValueValid <= min(lossValid):\n",
    "                    bestModel =  copy.deepcopy(graphFilter)\n",
    "            else:\n",
    "                bestModel =  copy.deepcopy(graphFilter)\n",
    "                    \n",
    "        batch+=1\n",
    "        \n",
    "    epoch+=1\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "################################\n",
    "############# PLOT #############\n",
    "################################\n",
    " \n",
    "plt.plot(lossTrain)\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Step')\n",
    "plt.show()\n",
    "   \n",
    "################################\n",
    "########## EVALUATION ##########\n",
    "################################\n",
    "\n",
    "print(\"Final evaluation results\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    yHatTest = graphFilter(xTest)\n",
    "lossTestLast = loss(yHatTest, yTest)\n",
    "lossTestLast = lossTestLast.item()\n",
    "with torch.no_grad():\n",
    "    yHatTest = bestModel(xTest)\n",
    "lossTestBest = loss(yHatTest, yTest)\n",
    "lossTestBest = lossTestBest.item()\n",
    "\n",
    "print(\" Graph Filter: %6.4f [Best]\" % (\n",
    "                    lossTestBest) + \" %6.4f [Last]\" % (\n",
    "                    lossTestLast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056726d",
   "metadata": {},
   "source": [
    "# Implmenting the Graph Perceptron Architecture;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphPerceptron(nn.Module):\n",
    "    def __init__(self, gso, k, sigma):\n",
    "        super().__init__()\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        self.weight = nn.Parameter(torch.randn(self.k))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = FilterFunction(self.weight, self.gso, x)\n",
    "        y = self.sigma(y)\n",
    "        return y    \n",
    "    \n",
    "graphPerceptron = GraphPerceptron(S, 8, nn.ReLU())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e9d97",
   "metadata": {},
   "source": [
    "# Training of the Graph Perceptron;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationInterval = 5\n",
    "\n",
    "nEpochs = 30\n",
    "batchSize = 200\n",
    "learningRate = 0.05\n",
    "\n",
    "nValid = int(np.floor(0.01*nTrain))\n",
    "xValid = xTrain[0:nValid,:]\n",
    "yValid = yTrain[0:nValid,:]\n",
    "xTrain = xTrain[nValid:,:]\n",
    "yTrain = yTrain[nValid:,:]\n",
    "nTrain = xTrain.shape[0]\n",
    "\n",
    "# Declaring the optimizers for each architectures\n",
    "optimizer = optim.Adam(graphPerceptron.parameters(), lr=learningRate)\n",
    "\n",
    "if nTrain < batchSize:\n",
    "    nBatches = 1\n",
    "    batchSize = [nTrain]\n",
    "elif nTrain % batchSize != 0:\n",
    "    nBatches = np.ceil(nTrain/batchSize).astype(np.int64)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "    while sum(batchSize) != nTrain:\n",
    "        batchSize[-1] -= 1\n",
    "else:\n",
    "    nBatches = np.int(nTrain/batchSize)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "batchIndex = np.cumsum(batchSize).tolist()\n",
    "batchIndex = [0] + batchIndex\n",
    "\n",
    "epoch = 0 # epoch counter\n",
    "\n",
    "# Store the training...\n",
    "lossTrain = dict()\n",
    "lossValid = dict()\n",
    "# ...and test variables\n",
    "lossTestBest = dict()\n",
    "lossTestLast = dict()\n",
    "\n",
    "bestModel = dict()\n",
    "\n",
    "lossTrain = []\n",
    "lossValid = []\n",
    "    \n",
    "while epoch < nEpochs:\n",
    "    randomPermutation = np.random.permutation(nTrain)\n",
    "    idxEpoch = [int(i) for i in randomPermutation]\n",
    "    print(\"\")\n",
    "    print(\"Epoch %d\" % (epoch+1))\n",
    "\n",
    "    batch = 0 \n",
    "    \n",
    "    while batch < nBatches:\n",
    "        # Determine batch indices\n",
    "        thisBatchIndices = idxEpoch[batchIndex[batch]\n",
    "                                    : batchIndex[batch+1]]\n",
    "        \n",
    "        # Get the samples in this batch\n",
    "        xTrainBatch = xTrain[thisBatchIndices,:]\n",
    "        yTrainBatch = yTrain[thisBatchIndices,:]\n",
    "\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            print(\"\")\n",
    "            print(\"    (E: %2d, B: %3d)\" % (epoch+1, batch+1),end = ' ')\n",
    "            print(\"\")\n",
    "        \n",
    "       \n",
    "        # Reset gradients\n",
    "        graphPerceptron.zero_grad()\n",
    "\n",
    "        # Obtain the output of the architectures\n",
    "        yHatTrainBatch = graphPerceptron(xTrainBatch)\n",
    "\n",
    "        # Compute loss\n",
    "        lossValueTrain = loss(yHatTrainBatch, yTrainBatch)\n",
    "\n",
    "        # Compute gradients\n",
    "        lossValueTrain.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        lossTrain += [lossValueTrain.item()]\n",
    "        \n",
    "        # Print:\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            with torch.no_grad():\n",
    "                # Obtain the output of the GNN\n",
    "                yHatValid = graphPerceptron(xValid)\n",
    "    \n",
    "            # Compute loss\n",
    "            lossValueValid = loss(yHatValid, yValid)\n",
    "            \n",
    "            lossValid += [lossValueValid.item()]\n",
    "\n",
    "            print(\"\\t Graph Perceptron: %6.4f [T]\" % (\n",
    "                    lossValueTrain) + \" %6.4f [V]\" % (\n",
    "                    lossValueValid))\n",
    "            \n",
    "            # Saving the best model so far\n",
    "            if len(lossValid) > 1:\n",
    "                if lossValueValid <= min(lossValid):\n",
    "                    bestModel =  copy.deepcopy(graphPerceptron)\n",
    "            else:\n",
    "                bestModel =  copy.deepcopy(graphPerceptron)\n",
    "                    \n",
    "        batch+=1\n",
    "        \n",
    "    epoch+=1\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "################################\n",
    "############# PLOT #############\n",
    "################################\n",
    " \n",
    "plt.plot(lossTrain)\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Step')\n",
    "plt.show()\n",
    "   \n",
    "################################\n",
    "########## EVALUATION ##########\n",
    "################################\n",
    "\n",
    "print(\"Final evaluation results\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    yHatTest = graphPerceptron(xTest)\n",
    "lossTestLast = loss(yHatTest, yTest)\n",
    "lossTestLast = lossTestLast.item()\n",
    "with torch.no_grad():\n",
    "    yHatTest = bestModel(xTest)\n",
    "lossTestBest = loss(yHatTest, yTest)\n",
    "lossTestBest = lossTestBest.item()\n",
    "\n",
    "print(\" Graph Perceptron: %6.4f [Best]\" % (\n",
    "                    lossTestBest) + \" %6.4f [Last]\" % (\n",
    "                    lossTestLast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d6166",
   "metadata": {},
   "source": [
    "# Implementing the Multi-layer GNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLGNN(nn.Module):\n",
    "    def __init__(self, gso, l, k, sigma):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for layer in range(l):\n",
    "            layers.append(GraphPerceptron(gso, k[layer], sigma))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        return y\n",
    "\n",
    "MLGNN = MLGNN(S, 2, [8, 1], nn.ReLU()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183b018",
   "metadata": {},
   "source": [
    "# Training of the Multi-layer GNN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationInterval = 5\n",
    "\n",
    "nEpochs = 30\n",
    "batchSize = 200\n",
    "learningRate = 0.05\n",
    "\n",
    "nValid = int(np.floor(0.01*nTrain))\n",
    "xValid = xTrain[0:nValid,:]\n",
    "yValid = yTrain[0:nValid,:]\n",
    "xTrain = xTrain[nValid:,:]\n",
    "yTrain = yTrain[nValid:,:]\n",
    "nTrain = xTrain.shape[0]\n",
    "\n",
    "# Declaring the optimizers for each architectures\n",
    "optimizer = optim.Adam(MLGNN.parameters(), lr=learningRate)\n",
    "\n",
    "if nTrain < batchSize:\n",
    "    nBatches = 1\n",
    "    batchSize = [nTrain]\n",
    "elif nTrain % batchSize != 0:\n",
    "    nBatches = np.ceil(nTrain/batchSize).astype(np.int64)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "    while sum(batchSize) != nTrain:\n",
    "        batchSize[-1] -= 1\n",
    "else:\n",
    "    nBatches = np.int(nTrain/batchSize)\n",
    "    batchSize = [batchSize] * nBatches\n",
    "batchIndex = np.cumsum(batchSize).tolist()\n",
    "batchIndex = [0] + batchIndex\n",
    "\n",
    "epoch = 0 # epoch counter\n",
    "\n",
    "# Store the training...\n",
    "lossTrain = dict()\n",
    "lossValid = dict()\n",
    "# ...and test variables\n",
    "lossTestBest = dict()\n",
    "lossTestLast = dict()\n",
    "\n",
    "bestModel = dict()\n",
    "\n",
    "lossTrain = []\n",
    "lossValid = []\n",
    "    \n",
    "while epoch < nEpochs:\n",
    "    randomPermutation = np.random.permutation(nTrain)\n",
    "    idxEpoch = [int(i) for i in randomPermutation]\n",
    "    print(\"\")\n",
    "    print(\"Epoch %d\" % (epoch+1))\n",
    "\n",
    "    batch = 0 \n",
    "    \n",
    "    while batch < nBatches:\n",
    "        # Determine batch indices\n",
    "        thisBatchIndices = idxEpoch[batchIndex[batch]\n",
    "                                    : batchIndex[batch+1]]\n",
    "        \n",
    "        # Get the samples in this batch\n",
    "        xTrainBatch = xTrain[thisBatchIndices,:]\n",
    "        yTrainBatch = yTrain[thisBatchIndices,:]\n",
    "\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            print(\"\")\n",
    "            print(\"    (E: %2d, B: %3d)\" % (epoch+1, batch+1),end = ' ')\n",
    "            print(\"\")\n",
    "        \n",
    "       \n",
    "        # Reset gradients\n",
    "        MLGNN.zero_grad()\n",
    "\n",
    "        # Obtain the output of the architectures\n",
    "        yHatTrainBatch = MLGNN(xTrainBatch)\n",
    "\n",
    "        # Compute loss\n",
    "        lossValueTrain = loss(yHatTrainBatch, yTrainBatch)\n",
    "\n",
    "        # Compute gradients\n",
    "        lossValueTrain.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        lossTrain += [lossValueTrain.item()]\n",
    "        \n",
    "        # Print:\n",
    "        if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "            with torch.no_grad():\n",
    "                # Obtain the output of the GNN\n",
    "                yHatValid = MLGNN(xValid)\n",
    "    \n",
    "            # Compute loss\n",
    "            lossValueValid = loss(yHatValid, yValid)\n",
    "            \n",
    "            lossValid += [lossValueValid.item()]\n",
    "\n",
    "            print(\"\\t MLGNN: %6.4f [T]\" % (\n",
    "                    lossValueTrain) + \" %6.4f [V]\" % (\n",
    "                    lossValueValid))\n",
    "            \n",
    "            # Saving the best model so far\n",
    "            if len(lossValid) > 1:\n",
    "                if lossValueValid <= min(lossValid):\n",
    "                    bestModel =  copy.deepcopy(MLGNN)\n",
    "            else:\n",
    "                bestModel =  copy.deepcopy(MLGNN)\n",
    "                    \n",
    "        batch+=1\n",
    "        \n",
    "    epoch+=1\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "################################\n",
    "############# PLOT #############\n",
    "################################\n",
    " \n",
    "plt.plot(lossTrain)\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Step')\n",
    "plt.show()\n",
    "   \n",
    "################################\n",
    "########## EVALUATION ##########\n",
    "################################\n",
    "\n",
    "print(\"Final evaluation results\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    yHatTest = MLGNN(xTest)\n",
    "lossTestLast = loss(yHatTest, yTest)\n",
    "lossTestLast = lossTestLast.item()\n",
    "with torch.no_grad():\n",
    "    yHatTest = bestModel(xTest)\n",
    "lossTestBest = loss(yHatTest, yTest)\n",
    "lossTestBest = lossTestBest.item()\n",
    "\n",
    "print(\" MLGNN: %6.4f [Best]\" % (\n",
    "                    lossTestBest) + \" %6.4f [Last]\" % (\n",
    "                    lossTestLast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46486542",
   "metadata": {},
   "source": [
    "# Multiple Feature Filters and GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterFunction(h, S, x):\n",
    "    \n",
    "    # Number of output features\n",
    "    F = h.shape[0]\n",
    "    \n",
    "    # Number of filter taps\n",
    "    K = h.shape[1]\n",
    "    \n",
    "    # Number of input features\n",
    "    G = h.shape[2]\n",
    "    \n",
    "    # Number of nodes\n",
    "    N = S.shape[1]\n",
    "    \n",
    "    # Batch size\n",
    "    B = x.shape[0]\n",
    "\n",
    "    # Create concatenation dimension and initialize concatenation tensor z\n",
    "    x = x.reshape([B, 1, G, N])\n",
    "    S = S.reshape([1, N, N])\n",
    "    z = x\n",
    "    \n",
    "    # Loop over the number of filter taps\n",
    "    for k in range(1, K):\n",
    "        \n",
    "        # S*x\n",
    "        x = torch.matmul(x, S)\n",
    "        \n",
    "        # Reshape\n",
    "        xS = x.reshape([B, 1, G, N])\n",
    "        \n",
    "        # Concatenate\n",
    "        z = torch.cat((z, xS), dim=1)\n",
    "    \n",
    "    # Multiply by h\n",
    "    y = torch.matmul(z.permute(0, 3, 1, 2).reshape([B, N, K*G]), h.reshape([F, K*G]).permute(1, 0)).permute(0, 2, 1)\n",
    "    return y\n",
    "\n",
    "class GraphFilter(nn.Module):\n",
    "    def __init__(self, gso, k, f_in, f_out):\n",
    "        super().__init__()\n",
    "        self.gso = torch.tensor(gso)\n",
    "        self.n = gso.shape[0]\n",
    "        self.k = k\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.weight = nn.Parameter(torch.randn(self.f_out, self.k, self.f_in))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.f_in * self.k)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return FilterFunction(self.weight, self.gso, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
